{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = \"./data/protein_info.csv\"\n",
    "OUTPUT_PATH = \"./esm_bluebert/esm_embeddings.npz\"\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"facebook/esm2_t30_150M_UR50D\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"Load ESM model and tokenizer\"\"\"\n",
    "    print(f\"Loading model {model_name} on {device}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model = model.half() if device == \"cuda\" else model\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_protein_data(data_path):\n",
    "    \"\"\"Load and preprocess protein data\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.drop_duplicates(subset=[\"NCBI_gene_id\"], keep=\"first\")\n",
    "    gene_ids = df[\"NCBI_gene_id\"].astype(str).tolist()\n",
    "    sequences = df[\"Sequence\"].astype(str).tolist()\n",
    "    print(f\"Found {len(gene_ids)} unique proteins\")\n",
    "    return gene_ids, sequences\n",
    "\n",
    "def generate_embeddings(batch_sequences, tokenizer, model, device, max_length):\n",
    "    \"\"\"Generate embeddings for a batch of protein sequences\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            batch_sequences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if device == \"cuda\":\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "            else:\n",
    "                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().float().numpy()\n",
    "        return embeddings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_embeddings(output_path, gene_ids, embeddings):\n",
    "    \"\"\"Save embeddings to file\"\"\"\n",
    "    print(f\"Saving embeddings to {output_path}...\")\n",
    "    np.savez(\n",
    "        output_path,\n",
    "        gene_ids=np.array(gene_ids),\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "def embeddings_exist(output_path):\n",
    "    \"\"\"Check if embeddings file already exists\"\"\"\n",
    "    return os.path.exists(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model facebook/esm2_t30_150M_UR50D on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/protein_info.csv...\n",
      "Found 9820 unique proteins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 9820/9820 [03:25<00:00, 47.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to ./esm_bluebert/esm_embeddings.npz...\n",
      "Successfully processed 9820 proteins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Check if embeddings already exist\n",
    "    if embeddings_exist(OUTPUT_PATH):\n",
    "        print(f\"Embeddings already exist at {OUTPUT_PATH}. Skipping generation.\")\n",
    "        return\n",
    "    \n",
    "    # Load model and data\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, DEVICE)\n",
    "    gene_ids, sequences = load_protein_data(DATA_PATH)\n",
    "    \n",
    "    # Process sequences in batches\n",
    "    all_embeddings = []\n",
    "    valid_gene_ids = []\n",
    "    \n",
    "    progress = tqdm(total=len(sequences), desc=\"Processing proteins\")\n",
    "    \n",
    "    for i in range(0, len(sequences), BATCH_SIZE):\n",
    "        batch_seqs = sequences[i:i+BATCH_SIZE]\n",
    "        batch_ids = gene_ids[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Filter out invalid sequences\n",
    "        valid_indices = [idx for idx, seq in enumerate(batch_seqs) \n",
    "                        if isinstance(seq, str) and len(seq.strip()) > 0]\n",
    "        valid_seqs = [batch_seqs[idx] for idx in valid_indices]\n",
    "        valid_ids = [batch_ids[idx] for idx in valid_indices]\n",
    "        \n",
    "        if not valid_seqs:\n",
    "            progress.update(len(batch_seqs))\n",
    "            continue\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = generate_embeddings(\n",
    "            valid_seqs, \n",
    "            tokenizer, \n",
    "            model, \n",
    "            DEVICE, \n",
    "            MAX_SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        if embeddings is not None and len(embeddings) > 0:\n",
    "            all_embeddings.append(embeddings)\n",
    "            valid_gene_ids.extend(valid_ids)\n",
    "        \n",
    "        progress.update(len(batch_seqs))\n",
    "    \n",
    "    progress.close()\n",
    "    \n",
    "    # Check if any embeddings were generated\n",
    "    if len(all_embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings generated. Check input data and model.\")\n",
    "    \n",
    "    # Save results\n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    save_embeddings(OUTPUT_PATH, valid_gene_ids, final_embeddings)\n",
    "    \n",
    "    print(f\"Successfully processed {len(valid_gene_ids)} proteins\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS177",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
