{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = \"./data/protein_info.csv\"     \n",
    "OUTPUT_PATH = \"./esm_bluebert/bluebert_embeddings.npz\"\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"\n",
    "MAX_SEQ_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load and return the tokenizer and model.\"\"\"\n",
    "    print(f\"Loading model {MODEL_NAME} on {DEVICE}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"Load and preprocess the protein data.\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.drop_duplicates(subset=[\"NCBI_gene_id\"], keep=\"first\")\n",
    "    \n",
    "    gene_ids = df[\"NCBI_gene_id\"].astype(str).tolist()\n",
    "    texts = df[\"Protein names\"].astype(str).tolist()\n",
    "    \n",
    "    print(f\"Found {len(gene_ids)} unique proteins\")\n",
    "    return gene_ids, texts\n",
    "\n",
    "def generate_text_embeddings(tokenizer, model, batch_texts):\n",
    "    \"\"\"Generate embeddings for a batch of texts.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            add_special_tokens=True\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if DEVICE == \"cuda\":\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().float().numpy()\n",
    "        return embeddings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_texts_in_batches(tokenizer, model, gene_ids, texts):\n",
    "    \"\"\"Process texts in batches and generate embeddings.\"\"\"\n",
    "    all_embeddings = []\n",
    "    valid_gene_ids = []\n",
    "    \n",
    "    progress = tqdm(total=len(texts), desc=\"Processing texts\")\n",
    "    \n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch_texts = texts[i:i+BATCH_SIZE]\n",
    "        batch_ids = gene_ids[i:i+BATCH_SIZE]\n",
    "        \n",
    "        valid_indices = [idx for idx, text in enumerate(batch_texts) \n",
    "                        if isinstance(text, str) and len(text.strip()) > 0]\n",
    "        valid_texts = [batch_texts[idx] for idx in valid_indices]\n",
    "        valid_ids = [batch_ids[idx] for idx in valid_indices]\n",
    "        \n",
    "        if not valid_texts:\n",
    "            progress.update(len(batch_texts))\n",
    "            continue\n",
    "        \n",
    "        embeddings = generate_text_embeddings(tokenizer, model, valid_texts)\n",
    "        \n",
    "        if embeddings is not None and len(embeddings) > 0:\n",
    "            all_embeddings.append(embeddings)\n",
    "            valid_gene_ids.extend(valid_ids)\n",
    "        \n",
    "        progress.update(len(batch_texts))\n",
    "    \n",
    "    progress.close()\n",
    "    \n",
    "    if len(all_embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings generated. Check input data and model.\")\n",
    "    \n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    return valid_gene_ids, final_embeddings\n",
    "\n",
    "def save_embeddings(output_path, gene_ids, embeddings):\n",
    "    \"\"\"Save embeddings to a file.\"\"\"\n",
    "    print(f\"Saving embeddings to {output_path}...\")\n",
    "    np.savez(\n",
    "        output_path,\n",
    "        gene_ids=np.array(gene_ids),\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    print(f\"Successfully processed {len(gene_ids)} protein names\")\n",
    "\n",
    "def embeddings_exist(output_path):\n",
    "    \"\"\"Check if embeddings file already exists.\"\"\"\n",
    "    return os.path.exists(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 on cuda...\n",
      "Loading data from ./data/protein_info.csv...\n",
      "Found 9820 unique proteins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 9820/9820 [00:05<00:00, 1642.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to ./esm_bluebert/bluebert_embeddings.npz...\n",
      "Successfully processed 9820 protein names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Check if embeddings already exist\n",
    "    if embeddings_exist(OUTPUT_PATH):\n",
    "        print(f\"Embeddings already exist at {OUTPUT_PATH}. Skipping generation.\")\n",
    "        return\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    gene_ids, texts = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    # Process texts and generate embeddings\n",
    "    valid_gene_ids, final_embeddings = process_texts_in_batches(\n",
    "        tokenizer, model, gene_ids, texts\n",
    "    )\n",
    "    \n",
    "    # Save embeddings\n",
    "    save_embeddings(OUTPUT_PATH, valid_gene_ids, final_embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS177",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
